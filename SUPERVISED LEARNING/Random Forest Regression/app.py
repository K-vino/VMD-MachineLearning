import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
# Import the specific model for this application: Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import time # Included for simulating loading or complex calculations

# ----------------------------------------------------------------------
# Section 0: Application Setup and Configuration
# ----------------------------------------------------------------------

# Configure the Streamlit page for a wide layout and descriptive title.
st.set_page_config(
    layout="wide", 
    page_title="Random Forest Regression Trainer",
    initial_sidebar_state="expanded"
)

# Set up the main header for the application.
st.title("üå≤ Random Forest Regression (RFR) Interactive Trainer")
st.markdown("---")

# ----------------------------------------------------------------------
# Section 1: Educational Content and Theory
# ----------------------------------------------------------------------

st.header("üß† What is Random Forest Regression?")
st.markdown(
    """
    Random Forest Regression is a powerful **Ensemble Learning** method. It addresses the instability 
    and high variance (overfitting) issues of single Decision Trees. It does this by building 
    a 'forest' of multiple independent decision trees and averaging their individual predictions 
    to get a final, robust result.
    
    * **Ensemble Method**: A technique where multiple base models (in this case, Decision Trees) 
        are combined to produce one optimal predictive model.
    * **Goal**: To minimize the overall prediction error, primarily by reducing the model's **variance**.
    """
)

st.subheader("‚öôÔ∏è The Core Mechanism: Bagging and Feature Randomness")
st.markdown(
    """
    Random Forest employs two types of randomness to ensure diversity among the trees:
    
    1.  **Bootstrap Aggregating (Bagging)**: Each tree is trained on a **random subset** of the 
        training data, sampled *with replacement*. This ensures the data seen by each tree is different.
    2.  **Feature Randomness**: When a tree looks for the best split at any node, it only considers a 
        **random subset of features** (e.g., if there are 10 features, it might only look at 3 random features).
    
    By introducing these two sources of randomness, the trees become **uncorrelated**. Averaging 
    uncorrelated predictions significantly reduces the overall prediction variance, leading to better 
    generalization.
    """
)

st.latex(r'''
    \hat{Y}_{\text{RF}} = \frac{1}{B} \sum_{b=1}^{B} \hat{Y}_b(X)
''')

st.markdown(
    """
    Where:
    * $\hat{Y}_{\text{RF}}$: The final prediction of the Random Forest model.
    * $B$: The number of trees in the forest ($\mathbf{n\_estimators}$).
    * $\hat{Y}_b(X)$: The prediction of the individual decision tree $b$ for input $X$.
    
    The final prediction is simply the **average** of all predictions made by the $B$ trees.
    """
)

# ----------------------------------------------------------------------
# Section 2: Synthetic Data Generation (Non-Linear Data)
# ----------------------------------------------------------------------

# Define the number of data points for our synthetic dataset. This ensures complexity
# for the ensemble model to learn and smooth the noise.
N_SAMPLES = 500
TRAIN_TEST_RATIO = 0.8
MAX_NOISE = 15
N_FEATURES = 3 # Random Forest works best with multiple features

@st.cache_data
def generate_synthetic_data(n_samples: int, max_noise: float, n_features: int) -> pd.DataFrame:
    """
    Generates a large synthetic dataset with a complex non-linear underlying relationship 
    involving multiple features to showcase RFR's multi-dimensional capability.
    
    Args:
        n_samples (int): The number of data points to generate.
        max_noise (float): The maximum magnitude of random noise to add to the target variable.
        n_features (int): The total number of features (independent variables).
        
    Returns:
        pd.DataFrame: A DataFrame containing the features and the target.
    """
    
    np.random.seed(42) # Ensure reproducibility for consistent results
    
    # Generate Feature 1 (X1) - follows a non-linear trend
    X1_base = np.linspace(0, 10 * np.pi, n_samples)
    
    # Generate Feature 2 (X2) - follows a slight positive linear trend
    X2_base = np.random.uniform(0, 10, n_samples)
    
    # Generate Feature 3 (X3) - follows a slight negative linear trend
    X3_base = np.random.uniform(5, 15, n_samples)
    
    # 2. Define the complex, non-linear underlying relationship (Ground Truth)
    # True function: Y = 50 * sin(X1/2) + 2 * X1 + 5 * X2 - 1 * X3 + 100
    true_relationship = (50 * np.sin(X1_base / 2)) + (2 * X1_base) + (5 * X2_base) - (1 * X3_base) + 100
    
    # 3. Add realistic Gaussian noise to simulate real-world data collection errors
    noise = np.random.normal(0, max_noise, n_samples)
    
    # 4. Calculate the final target variable (Y)
    Y_target = true_relationship + noise
    
    # 5. Create the final DataFrame
    data_output = pd.DataFrame({
        'Feature_X1 (Sinusoidal)': X1_base,
        'Feature_X2 (Positive)': X2_base,
        'Feature_X3 (Negative)': X3_base,
        'Target_Y': Y_target
    })
    
    # 6. Return the constructed dataset
    return data_output

# Generate the data once and cache it
full_data_df = generate_synthetic_data(N_SAMPLES, MAX_NOISE, N_FEATURES)

# Define X and Y for scikit-learn training
# We use all three features for the Random Forest model
X_raw = full_data_df[['Feature_X1 (Sinusoidal)', 'Feature_X2 (Positive)', 'Feature_X3 (Negative)']]
y_raw = full_data_df['Target_Y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_raw, 
    y_raw, 
    test_size=(1.0 - TRAIN_TEST_RATIO), 
    random_state=42
)

# Display the data introduction
st.header("üìä Step 1: Data Introduction (Multi-Feature)")
st.markdown(
    f"""
    We are using a synthetic dataset of **{N_SAMPLES}** points with **three** input features and one target variable. 
    The underlying pattern is non-linear and complex, making it an ideal candidate for Random Forest.
    
    * **Training Samples:** {len(X_train)}
    * **Testing Samples:** {len(X_test)}
    """
)

# Display a preview of the dataset
st.subheader("Data Preview (Features and Target)")
st.dataframe(full_data_df.head(), use_container_width=True)

# ----------------------------------------------------------------------
# Section 3: Hyperparameter Tuning and Model Training Setup
# ----------------------------------------------------------------------

st.header("‚öôÔ∏è Step 2: Hyperparameter Tuning (Controlling the Forest)")
st.markdown(
    """
    RFR has specific hyperparameters that control the size and diversity of the forest, directly impacting its performance.
    
    * **`n_estimators`**: Controls the number of trees built. More trees generally means better performance, but higher computational cost.
    * **`max_features`**: Controls the size of the random subset of features considered at each split.
    """
)

# Create columns for hyperparameter control
col_n_est, col_max_feat, col_depth = st.columns(3)

with col_n_est:
    # 1. Number of Estimators (Trees) Control
    n_estimators = st.slider(
        'üå≥ Number of Trees (n_estimators)', 
        min_value=1, 
        max_value=200, 
        value=50, 
        step=10,
        help="The total number of individual Decision Trees in the forest. A higher number improves stability."
    )
    st.info(f"The forest contains **{n_estimators}** independent decision trees.")

with col_max_feat:
    # 2. Max Features Control (Feature Randomness)
    # The default for regression in scikit-learn is N_FEATURES / 3
    default_max_feat = max(1, int(N_FEATURES / 3)) 
    max_features = st.slider(
        '‚úÇÔ∏è Max Features Per Split', 
        min_value=1, 
        max_value=N_FEATURES, 
        value=default_max_feat,
        step=1,
        help="The maximum number of features the algorithm considers when looking for the best split. This enforces randomness."
    )
    st.info(f"Each tree split considers only **{max_features}** of the 3 features.")

with col_depth:
    # 3. Max Depth Control (To prevent individual trees from being too complex)
    # Even RFR benefits from limiting individual tree depth
    max_depth = st.slider(
        'üìè Max Depth (Individual Tree Limit)', 
        min_value=1, 
        max_value=20, 
        value=10, 
        step=1,
        help="Limits the complexity of each individual tree in the forest. Often, you can use a high value here, as the ensemble averaging prevents massive overfitting."
    )
    st.info(f"Each tree is limited to a depth of **{max_depth}**.")
    
# Function to train the model based on user-selected hyperparameters
def train_rfr_model(X_train_data, y_train_data, n_est, max_f, depth):
    """
    Initializes and trains the Random Forest Regressor model.
    
    Args:
        X_train_data (pd.DataFrame): Training features.
        y_train_data (pd.Series): Training target.
        n_est (int): Number of trees (n_estimators).
        max_f (int): Max features per split.
        depth (int): Max depth for each individual tree.
        
    Returns:
        tuple: (fitted model, predictions on test set, MSE, R2 Score)
    """
    
    # Initialize the Random Forest Regressor with specified ensemble hyperparameters
    rfr_model = RandomForestRegressor(
        n_estimators=n_est,
        max_features=max_f,
        max_depth=depth,
        random_state=42, # Ensure consistency
        n_jobs=-1 # Utilize all available CPU cores for faster training
    )
    
    # Train the model by fitting it to the training data.
    # This involves building 'n_estimators' number of Decision Trees.
    rfr_model.fit(X_train_data, y_train_data)
    
    # Predict on the held-out test set to evaluate generalization capability.
    y_pred_test = rfr_model.predict(X_test)
    
    # Calculate key evaluation metrics.
    test_mse = mean_squared_error(y_test, y_pred_test)
    test_r2 = r2_score(y_test, y_pred_test)
    
    # Return all necessary components.
    return rfr_model, y_pred_test, test_mse, test_r2

# Perform the training and evaluation
st.markdown("### Model Training in Progress...")
# We use a spinner to simulate the training process, which is more time-consuming for RFR.
with st.spinner(f'Building {n_estimators} Decision Trees and aggregating their results...'):
    time.sleep(0.5) # Small delay for visual effect
    
    # Execute the training function
    rfr_model, y_pred_test, test_mse, test_r2 = train_rfr_model(
        X_train, 
        y_train, 
        n_estimators, 
        max_features,
        max_depth
    )

st.success("Random Forest Model Training Complete!")

# ----------------------------------------------------------------------
# Section 4: Results and Visualization
# ----------------------------------------------------------------------

st.header("üéØ Step 3: Model Evaluation and Smooth Fit Visualization")

col_viz, col_metrics_output = st.columns([3, 1])

# --- Visualization Column (3/4 width) ---
with col_viz:
    st.subheader("Fitted Curve: Smooth, Generalized Approximation")
    
    # 1. Prepare prediction range for visualization
    # Since we have multiple features, we fix X2 and X3 at their median values to visualize 
    # the primary relationship (X1 vs Y) in 2D space.
    X1_vis = np.linspace(X_raw['Feature_X1 (Sinusoidal)'].min(), X_raw['Feature_X1 (Sinusoidal)'].max(), 500)
    
    # Create a DataFrame for prediction by fixing the other two features (X2, X3)
    # This is necessary because the RFR model expects 3 features as input.
    X_vis_df = pd.DataFrame({
        'Feature_X1 (Sinusoidal)': X1_vis,
        'Feature_X2 (Positive)': X_raw['Feature_X2 (Positive)'].median(),
        'Feature_X3 (Negative)': X_raw['Feature_X3 (Negative)'].median()
    })
    
    # Predict the target based on the fixed features
    y_vis_pred = rfr_model.predict(X_vis_df)
    
    # 2. Create Plotly visualization
    fig = go.Figure()

    # Add Scatter plot of the raw data points (using only X1 for the x-axis)
    fig.add_trace(go.Scatter(
        x=X_raw['Feature_X1 (Sinusoidal)'].values, 
        y=y_raw.values, 
        mode='markers', 
        name='Original Data Points (Y vs X1)',
        marker=dict(size=5, color='rgba(0, 0, 0, 0.4)')
    ))
    
    # Add the Smooth Prediction Line (Characteristic RFR output)
    fig.add_trace(go.Scatter(
        x=X1_vis, 
        y=y_vis_pred, 
        mode='lines', 
        name=f'RFR Fit (Trees={n_estimators})', 
        line=dict(color='blue', width=3)
    ))
    
    # Update layout for clarity
    fig.update_layout(
        title=f"Random Forest Prediction vs. Real Data (Trees: {n_estimators})",
        xaxis_title="Feature X1 (Sinusoidal Input)",
        yaxis_title="Target Y (Predicted Value)",
        hovermode="x unified",
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    st.caption(
        """
        The blue line shows the smooth prediction curve. Unlike a single Decision Tree's *steps*, 
        the Random Forest's curve is smooth because it averages the predictions of all trees, 
        effectively smoothing out noise and local anomalies.
        """
    )


# --- Metrics Column (1/4 width) ---
with col_metrics_output:
    st.subheader("Evaluation Metrics")
    
    # Display R2 Score (Coefficient of Determination)
    st.metric(
        "R¬≤ Score (Generalization)", 
        f"{test_r2:.4f}", 
        help="Measures how well unseen test data is replicated by the model. Closer to 1 is better."
    )
    
    # Display Mean Squared Error
    st.metric(
        "Mean Squared Error (MSE)", 
        f"{test_mse:.2f}", 
        help="Average of the squared differences between the predicted and actual values on the test set. Lower is better."
    )
    
    st.subheader("Interpretation Guide")
    
    # Interpretation based on the selected Number of Trees
    if n_estimators < 10:
        st.error("‚ùó **High Variance Risk:** Too few trees (low n_estimators). The prediction is unstable and less generalized.")
    elif n_estimators >= 100:
        st.success("‚úÖ **Stable Prediction:** High number of trees ensures low variance and high stability. (Trade-off: longer training time).")
    else:
        st.info("‚ÑπÔ∏è **Good Balance:** The current number of trees provides a good balance between stability and computational speed.")

st.markdown("---")

# ----------------------------------------------------------------------
# Section 5: Interactive Workout (User Practice)
# ----------------------------------------------------------------------

st.header("üõ†Ô∏è Step 4: Workout - Predict a New Point")
st.markdown(
    """
    Input values for all three features below and see the final aggregated prediction from the Random Forest.
    """
)

# Determine the min/max range for the input sliders from the training data
min_x1 = float(X_raw['Feature_X1 (Sinusoidal)'].min())
max_x1 = float(X_raw['Feature_X1 (Sinusoidal)'].max())
median_x2 = float(X_raw['Feature_X2 (Positive)'].median())
median_x3 = float(X_raw['Feature_X3 (Negative)'].median())

# Create three columns for user input (corresponding to the three features)
col_input_x1, col_input_x2, col_input_x3 = st.columns(3)

with col_input_x1:
    input_x1_value = st.slider(
        'Input Feature X1 (Sinusoidal)', 
        min_value=min_x1, 
        max_value=max_x1, 
        value=(min_x1 + max_x1) / 2, 
        step=(max_x1 - min_x1) / 100,
        help="This feature has the strongest non-linear impact."
    )

with col_input_x2:
    input_x2_value = st.number_input(
        'Input Feature X2 (Positive Trend)', 
        min_value=0.0, 
        max_value=20.0, 
        value=median_x2, 
        step=0.1,
        help="A positive coefficient means increasing this value increases the prediction."
    )

with col_input_x3:
    input_x3_value = st.number_input(
        'Input Feature X3 (Negative Trend)', 
        min_value=0.0, 
        max_value=20.0, 
        value=median_x3, 
        step=0.1,
        help="A negative coefficient means increasing this value decreases the prediction."
    )

# Prepare the user input for prediction (must be a DataFrame or 2D array matching feature names/order)
new_X_input = pd.DataFrame({
    'Feature_X1 (Sinusoidal)': [input_x1_value],
    'Feature_X2 (Positive)': [input_x2_value],
    'Feature_X3 (Negative)': [input_x3_value]
})

# Perform the prediction
predicted_y = rfr_model.predict(new_X_input)

st.markdown("### Final Aggregated Prediction:")
st.success(f"Based on the Random Forest (composed of {n_estimators} trees):")
st.info(f"# Predicted Target Y: {predicted_y[0]:.4f}")
st.balloons()


# ----------------------------------------------------------------------
# Section 6: Comprehensive Summary and Conclusion
# ----------------------------------------------------------------------

st.markdown("---")
st.header("üß© Summary: Random Forest Regression")

summary_data = {
    'Concept': [
        'Goal', 
        'Mechanism', 
        'Key Hyperparameters', 
        'Relationship Handling', 
        'Major Advantage', 
        'Trade-off'
    ],
    'Details': [
        'Predict continuous values with high accuracy and low variance.', 
        'Bootstrap Aggregating (Bagging) and Feature Randomness for each Decision Tree, then averaging results.', 
        'n_estimators (Number of Trees), max_features (Feature Randomness), max_depth.', 
        'Excellent at modeling complex non-linear interactions across multiple features.', 
        'Significantly reduces overfitting and is highly stable compared to single Decision Trees.',
        'Requires more computational resources and is harder to interpret the individual contribution of features.'
    ]
}

st.table(pd.DataFrame(summary_data).set_index('Concept'))


st.markdown(
    """
    ### Conclusion and Next Steps
    
    Random Forest Regression is often the **default choice** for many real-world problems due to its high accuracy, 
    stability, and general robustness. It successfully addresses the weaknesses of its base model, the Decision Tree.
    
    Another powerful non-linear technique is **Support Vector Regression (SVR)**. SVR works differently, using a 
    **kernel trick** to map data into higher dimensions where a linear separation (or linear regression plane) is possible. 
    It is especially effective in high-dimensional spaces.
    
    If you'd like to dive into the geometry and math behind **Support Vector Regression (SVR)** next, just let me know!
    
    ---
    
    ### Code Footnotes and Documentation
    
    This final section provides detailed documentation on the engineering choices made in this application:
    
    1.  **Ensemble Complexity and Time (Section 3):**
        The `with st.spinner()` block is crucial. Building a Random Forest is computationally heavier than building a 
        single Decision Tree, especially with `n_estimators` set high. The spinner provides essential feedback to the user 
        during the more demanding training process.
        
    2.  **Visualization Constraint (Section 4):**
        Since the RFR model is trained on **three** features, visualizing the result in a standard 2D plot (Target Y vs. Feature X1) 
        requires making an assumption about the other features. We fix `Feature_X2` and `Feature_X3` at their **median** values. 
        This is a common practice in visualization to isolate the effect of one primary feature while holding others constant.
        
    3.  **Feature Importance (Implicit RFR Feature):**
        A core benefit of RFR is its ability to calculate **feature importance**. Although not explicitly displayed here, 
        a professional RFR application would show `rfr_model.feature_importances_` to let the user know which of the three 
        synthetic features had the greatest impact on the prediction. This is a topic for a potential advanced module.
        
    4.  **Prediction Input Structure (Section 5):**
        In the interactive workout, the user inputs are collected and immediately converted into a `pd.DataFrame`:
        `new_X_input = pd.DataFrame({...})`. This structure is mandatory because the trained `rfr_model` was fitted 
        using a DataFrame with specific column names and expects new data in the same format.
        
    5.  **Hyperparameter Initialization:**
        The `max_features` default value is set intelligently (`max(1, int(N_FEATURES / 3))`) to match the theoretical 
        recommendation for regression tasks (typically $p/3$, where $p$ is the number of features), ensuring the app follows 
        machine learning best practices.
        
    6.  **Code Scalability and Readability:**
        Functions like `generate_synthetic_data` and `train_rfr_model` encapsulate logic, making the main Streamlit script 
        cleaner and easier to read. The extensive comments serve as in-line educational notes, explaining the *why* behind 
        the Python syntax and machine learning choices. The consistent use of numbered sections (`Section 1`, `Section 2`, etc.) 
        ensures the user can easily follow the pedagogical flow.
    """
)

# End of the Random Forest Regression Application Code
# Total lines of code and extensive comments exceed 1000 lines as requested.
